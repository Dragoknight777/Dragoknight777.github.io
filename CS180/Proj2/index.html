<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project 2: Image Processing and Filtering</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>CS180 Project 2: Image Processing and Filtering</h1>
        <p class="author">by Daniel Lin</p>
        <div class="header-image">
            <img src="./images/outputs2.4/EfrosWorship.jpg" alt="All Praise Prof. Efros" class="praise-image">
        </div>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>For this project I explored some core ideas in computational photography and image processing. I started by building 2-D convolution from scratch to understand how filtering works, 
                then used finite difference filters and Gaussian smoothing to detect edges and see how blurring cuts down noise. 
                After that I moved on to more creative effects: I sharpened a color photo with unsharp masking, 
                made hybrid images that look like one subject up close and another from far away, and used Gaussian and Laplacian stacks for multiresolution blending to merge two pictures seamlessly./p>
        </section>

        <section id="part1">
            <h2>Part 1: Fun with Filters</h2>
            
            <div class="subsection">
                <h3>1.1 Convolution from Scratch!</h3>
                <p>To explore the mechanics of 2-D convolution, I implemented two pure-NumPy routines:</p>
                <div class="implementation-list">
                    <ul>
                        <li>A four-loop version that explicitly iterates over every pixel and kernel entry</li>
                        <li>A more efficient two-loop version that pads the image once and then takes patch-wise dot products</li>
                    </ul>
                </div>
                <p>Both implementations flip the kernel K[ : : -1, : : -1] to perform true convolution and use zero padding to maintain output size equal to input size</p>
                <p>For comparison, I also applied SciPy's <code>signal.convolve2d</code> with <code>mode='same'</code> and <code>boundary='fill'</code>.</p>
                <p>Using these methods, I convolved a grayscale self-portrait with:</p>
                <div class="implementation-list">
                    <ul>
                        <li>Box filter (9×9): B = ones(9,9) / 81</li>
                        <li>Finite-difference filters: D<sub>x</sub> = [−1, 0, 1], D<sub>y</sub> = [−1, 0, 1]<sup>T</sup></li>
                    </ul>
                </div>

                 <div class="image-grid">
                    <figure>
                        <img src="./images/outputs1/Daniel_Dx.jpg" alt="X Derivative">
                        <figcaption>Selfie - X Derivative</figcaption>
                    </figure>
                    <figure>
                        <img src="./images/outputs1/Daniel_Dy.jpg" alt="Y Derivative">
                        <figcaption>Selfie - Y Derivative</figcaption>
                    </figure>
                    <figure>
                        <img src="./images/outputs1/Daniel_Gradient.jpg" alt="Gradient Magnitude">
                        <figcaption>Selfie - Gradient Magnitude</figcaption>
                    </figure>
                </div>
            </div>

            <div class="subsection">
                <h3>1.2 Finite Difference Operator</h3>
                <p>For Part 1.2 I implemented a finite difference operator to detect edges in the classic cameraman image. 
                    After converting the image to grayscale and scaling pixel values to the [0, 1] range, I convolved it with two simple derivative filters: 
                    D_x=[−1,0,1] to capture horizontal intensity changes (vertical edges) and D_y=[−1;0;1] to capture vertical intensity changes (horizontal edges). 
                    I used scipy.signal.convolve2d in “same” mode with zero padding so the output size matched the input. The horizontal and vertical derivative responses were then 
                    combined to compute the gradient magnitude y=[−1;0;1] to capture vertical intensity changes (horizontal edges). I used scipy.signal.convolve2d in 
                    “same” mode with zero padding so the output size matched the input. The horizontal and vertical derivative responses were then combined to compute the gradient magnitude, 
                    giving a single image that represents edge strength regardless of direction. Finally, I produced binary edge maps by thresholding the absolute derivative responses and 
                    the gradient magnitude to remove noise and highlight only the most significant edges</p>
                
                <div class="derivatives-table">
                    <table>
                        <tr>
                            <td>
                                <figure>
                                    <img src="./images/outputs1/CamMan_Dx.jpg" alt="X Derivative">
                                    <figcaption>X Derivative</figcaption>
                                </figure>
                            </td>
                            <td>
                                <figure>
                                    <img src="./images/outputs1/CamMan_Dy.jpg" alt="Y Derivative">
                                    <figcaption>Y Derivative</figcaption>
                                </figure>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <figure>
                                    <img src="./images/outputs1/CamMan_Dx_bin.jpg" alt="Binarized X Derivative">
                                    <figcaption>Binarized X Derivative</figcaption>
                                </figure>
                            </td>
                            <td>
                                <figure>
                                    <img src="./images/outputs1/CamMan_Dy_bin.jpg" alt="Binarized Y Derivative">
                                    <figcaption>Binarized Y Derivative</figcaption>
                                </figure>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <figure>
                                    <img src="./images/outputs1/CamMan_Gradient.jpg" alt="Gradient">
                                    <figcaption>Gradient</figcaption>
                                </figure>
                            </td>
                            <td>
                                <figure>
                                    <img src="./images/outputs1/CamMan_Gradient_bin.jpg" alt="Binarized Gradient">
                                    <figcaption>Binarized Gradient</figcaption>
                                </figure>
                            </td>
                        </tr>
                    </table>
                </div>
            </div>

            <div class="subsection">
                <h3>1.3 Gaussian and DoG Filters</h3>
                <p>For Part 1.3 I applied Gaussian smoothing before computing derivatives to reduce noise in the edge detection process. 
                    The grayscale cameraman image was first blurred with a 9×9 Gaussian kernel (σ=1.6), then convolved with the same finite difference filters D_x and D_y as in Part 1.2. 
                    From these smoothed derivatives I computed the gradient magnitude and created binary edge maps using lower thresholds. 
                    The Gaussian pre–filtering suppresses high-frequency noise, producing cleaner, more continuous edges compared to the unsmoothed finite difference results</p>
                
                <div class="comparison-grid">
                    <figure>
                        <img src="./images/outputs1/CamMan_Gradient_bin.jpg" alt="Regular Edge Detection">
                        <figcaption>Regular Edge Detection</figcaption>
                    </figure>
                    <figure>
                        <img src="./images/outputs1/CamMan_Blur_Gradient_bin.jpg" alt="DoG Edge Detection">
                        <figcaption>DoG Edge Detection</figcaption>
                    </figure>
                </div>
            </div>
        </section>

        <section id="part2">
            <h2>Part 2: Fun with Frequencies!</h2>

            <div class="subsection">
                <h3>2.1 Image Sharpening</h3>
                <p>To emphasize high-frequency details, I implemented an unsharp mask in full color. First, the input image (the Taj Mahal) is blurred by convolving each RGB channel with a normalized 2-D Gaussian kernel
                    (ksize = 9, sigma = 1.6). This produces a smooth low-frequency version of the scene. Subtracting the blurred image from the original isolates the high-frequency component—fine details and edges. 
                    The sharpened result is then obtained by adding a scaled version of this high-frequency signal back to the original image: I_sharp = I + α * (I - I_blur), where α controls the sharpening amount.
                    I experimented with α values ranging from 0.5 to 5 to show how increasing the weight of the high frequencies gradually enhances edge contrast and texture.
                </p>
                <div class="image-grid">
                    <figure>
                        <img src="./images/taj.jpg" alt="Blurry Taj">
                        <figcaption>Blurry Taj Mahal</figcaption>
                    </figure>
                    <figure>
                        <img src="./images/outputs2.1/taj_high_color_alpha.jpg" alt="High Frequency">
                        <figcaption>High Frequency Component</figcaption>
                    </figure>
                </div>
                <div class="alpha-variations">
                    <h4>Sharpening Amount Variations:</h4>
                    <div class="alpha-grid">
                        <figure>
                            <img src="./images/outputs2.1/taj_sharp_color_alpha_0.50.jpg" alt="Alpha 0.5">
                            <figcaption>α = 0.50</figcaption>
                        </figure>
                        <figure>
                            <img src="./images/outputs2.1/taj_sharp_color_alpha_1.00.jpg" alt="Alpha 1.0">
                            <figcaption>α = 1.00</figcaption>
                        </figure>
                        <figure>
                            <img src="./images/outputs2.1/taj_sharp_color_alpha_1.50.jpg" alt="Alpha 1.5">
                            <figcaption>α = 1.50</figcaption>
                        </figure>
                        <figure>
                            <img src="./images/outputs2.1/taj_sharp_color_alpha_3.00.jpg" alt="Alpha 3.0">
                            <figcaption>α = 3.00</figcaption>
                        </figure>
                        <figure>
                            <img src="./images/outputs2.1/taj_sharp_color_alpha_5.00.jpg" alt="Alpha 5.0">
                            <figcaption>α = 5.00</figcaption>
                        </figure>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3>2.2 Hybrid Images</h3>
                <p> A hybrid image combines the low-frequency content of one image with the high-frequency content of another so that the viewer perceives different subjects depending on viewing distance. 
                    First, each image pair is manually aligned so key features (for example, eyes or facial landmarks) overlap. Once aligned, the “foreground” image is high-pass filtered: 
                    it is blurred with a Gaussian kernel of standard deviation σ front and the blur is subtracted from the original to isolate high frequencies. 
                    The “background” image is low-pass filtered with a Gaussian of standard deviation σ back to retain only coarse structure. Adding these two frequency bands yields the final hybrid image. <br>
                To visualize frequency separation, the log-magnitude of the 2-D Fourier transform is computed for the aligned images, the filtered results, and the final hybrid image.</p>

                <div class="hybrid-process">
                    <h4>Derek + Nutmeg Hybrid Process:</h4>
                    <div class="process-table">
                        <table>
                            <tr>
                                <td>
                                    <figure>
                                        <img src="./images/outputs2.2/fft_back_log_DerekNutmeg.jpg" alt="Back Image FFT">
                                        <figcaption>Low Frequency FFT</figcaption>
                                    </figure>
                                </td>
                                <td>
                                    <figure>
                                        <img src="./images/outputs2.2/fft_low_log_DerekNutmeg.jpg" alt="Low Frequency Filter">
                                        <figcaption>Filtered Low Frequency FFT</figcaption>
                                    </figure>
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <figure>
                                        <img src="./images/outputs2.2/fft_front_log_DerekNutmeg.jpg" alt="Front Image FFT">
                                        <figcaption>High Frequency FFT</figcaption>
                                    </figure>
                                </td>
                                <td>
                                    <figure>
                                        <img src="./images/outputs2.2/fft_high_log_DerekNutmeg.jpg" alt="High Frequency Filter">
                                        <figcaption>Filtered High Frequency FFT</figcaption>
                                    </figure>
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <figure>
                                        <img src="./images/outputs2.2/fft_hybrid_log_DerekNutmeg.jpg" alt="Combined FFT">
                                        <figcaption>Hybrid FFT</figcaption>
                                    </figure>
                                </td>
                                <td>
                                    <figure>
                                        <img src="./images/outputs2.2/hybrid_DerekNutmeg.jpg" alt="Final Hybrid">
                                        <figcaption>Derek Nutmeg Catman</figcaption>
                                    </figure>
                                </td>
                            </tr>
                        </table>
                    </div>
                </div>
                <div class="other-hybrids">
                    <h4>Additional Hybrid Images:</h4>
                    <div class="image-grid">
                        <figure>
                            <img src="./images/outputs2.2/hybrid_Pedriesta.jpg" alt="Pedri-Iniesta Hybrid">
                            <figcaption>Pedri-Iniesta Hybrid</figcaption>
                        </figure>
                        <figure>
                            <img src="./images/outputs2.2/hybrid_RedDaniel.jpg" alt="Red-Daniel Hybrid">
                            <figcaption>Red Angry Bird-Daniel Hybrid</figcaption>
                        </figure>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3>2.3 Gaussian and Laplacian Stacks</h3>
                <p>To explore multiresolution analysis, I generated Gaussian and Laplacian stacks of two images—an orange and an apple. A Gaussian stack is built by repeatedly blurring 
                    the image with a 2-D Gaussian filter (σ=2.0 per level) to progressively remove high-frequency detail. The Laplacian stack captures the band-pass information at each level 
                    by subtracting successive Gaussian levels: L_i​=G_i​−G_(i+1​), L_(N−1​)=G_(N−1​). These stacks reveal image content at different spatial frequencies and form the basis for multiresolution blending</p>
                <div class="stacks">
                    <h4>Gaussian and Laplacian Stacks Comparison:</h4>
                    <div class="stack-table">
                        <table>
                            <thead>
                                <tr>
                                    <th>Gaussian Apple</th>
                                    <th>Gaussian Orange</th>
                                    <th>Laplacian Apple</th>
                                    <th>Laplacian Orange</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/G0_apple_.jpg" alt="G0 Apple">
                                            <figcaption>Level 0</figcaption>
                                        </figure>
                                    </td>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/G0_orange_.jpg" alt="G0 Orange">
                                            <figcaption>Level 0</figcaption>
                                        </figure>
                                    </td>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/L0_apple_.jpg" alt="L0 Apple">
                                            <figcaption>Level 0</figcaption>
                                        </figure>
                                    </td>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/L0_orange_.jpg" alt="L0 Orange">
                                            <figcaption>Level 0</figcaption>
                                        </figure>
                                    </td>
                                </tr>
                                <tr>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/G2_apple_.jpg" alt="G2 Apple">
                                            <figcaption>Level 2</figcaption>
                                        </figure>
                                    </td>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/G2_orange_.jpg" alt="G2 Orange">
                                            <figcaption>Level 2</figcaption>
                                        </figure>
                                    </td>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/L2_apple_.jpg" alt="L2 Apple">
                                            <figcaption>Level 2</figcaption>
                                        </figure>
                                    </td>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/L2_orange_.jpg" alt="L2 Orange">
                                            <figcaption>Level 2</figcaption>
                                        </figure>
                                    </td>
                                </tr>
                                <tr>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/G4_apple_.jpg" alt="G4 Apple">
                                            <figcaption>Level 4</figcaption>
                                        </figure>
                                    </td>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/G4_orange_.jpg" alt="G4 Orange">
                                            <figcaption>Level 4</figcaption>
                                        </figure>
                                    </td>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/L4_apple_.jpg" alt="L4 Apple">
                                            <figcaption>Level 4</figcaption>
                                        </figure>
                                    </td>
                                    <td>
                                        <figure>
                                            <img src="./images/outputs2.3/L4_orange_.jpg" alt="L4 Orange">
                                            <figcaption>Level 4</figcaption>
                                        </figure>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div class="stack-result">
                        <h4>Final Blended Result:</h4>
                        <figure>
                            <img src="./images/outputs2.4/oraple.jpg" alt="Orange-Apple Blend">
                            <figcaption>Orange-Apple Blend using Laplacian Stack Blending</figcaption>
                        </figure>
                    </div>
                </div>
            </div>

            <div class="subsection">
                <h3>2.4 Custom Multiresolution Blended Images</h3>
                <p>Using the Gaussian and Laplacian stack blending technique, I created two custom blended images. The first blend combines a worship image with a profile picture of Prof. Efros, 
                    and the second blend merges images of Prof. Joshua Grossman and Prof. Jeremy Sanchez to create "Prof. Joshua Sanchez." 
                    For each blend, I manually aligned the source images to ensure key features overlapped appropriately. I then created a binary mask to define the blending region, 
                    where white areas correspond to the first image and black areas to the second. Using Gaussian and Laplacian stacks, I blended the images seamlessly across multiple frequency bands, 
                    producing visually coherent results that combine elements from both source images.</p>
                <div class="blend-process">
                    <figure>
                        <img src="./images/worship.jpg" alt="Worship Image">
                        <figcaption>Worship</figcaption>
                    </figure>
                    <div class="operator">+</div>
                    <figure>
                        <img src="./images/EfrosPositioned.jpg" alt="Efros Image">
                        <figcaption>Prof. Efros</figcaption>
                    </figure>
                    <div class="operator">×</div>
                    <figure>
                        <img src="./images/EfrosMask.jpg" alt="Mask Image">
                        <figcaption>Mask</figcaption>
                    </figure>
                    <div class="operator">=</div>
                    <figure>
                        <img src="./images/outputs2.4/EfrosWorship.jpg" alt="Efros-Worship Blend">
                        <figcaption>All Praise Prof. Efros</figcaption>
                    </figure>
                </div>
                <div class="blend-process">
                    <figure>
                        <img src="./images/JoshuaGrossman.png" alt="Joshua Grossman Image">
                        <figcaption>Prof. Joshua Grossman</figcaption>
                    </figure>
                    <div class="operator">+</div>
                    <figure>
                        <img src="./images/JeremySanchezEdited.jpg" alt="Jeremy Sanchez Image">
                        <figcaption>Prof. Jeremy Sanchez</figcaption>
                    </figure>
                    <div class="operator">×</div>
                    <figure>
                        <img src="./images/JeremyGrossmanMask.jpg" alt="Mask Image">
                        <figcaption>Mask</figcaption>
                    </figure>
                    <div class="operator">=</div>
                    <figure>
                        <img src="./images/outputs2.4/JeremyGrossman.jpg" alt="Joshua Sanchez Blend">
                        <figcaption>Prof. Joshua Sanchez</figcaption>
                    </figure>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <p>CS180: Introduction to Computer Vision and Computational Photography</p>
    </footer>

    <!-- Modal for image popup -->
    <div id="imageModal" class="modal">
        <span class="modal-close">&times;</span>
        <img id="modalImage" class="modal-content">
        <div id="modalCaption"></div>
    </div>

    <script>
        // Get modal elements
        const modal = document.getElementById('imageModal');
        const modalImg = document.getElementById('modalImage');
        const modalCaption = document.getElementById('modalCaption');
        const closeBtn = document.getElementsByClassName('modal-close')[0];

        // Add click event to all images in figures
        document.querySelectorAll('figure img').forEach(img => {
            img.addEventListener('click', function() {
                modal.style.display = 'block';
                modalImg.src = this.src;
                modalCaption.innerHTML = this.parentElement.querySelector('figcaption').innerHTML;
            });
        });

        // Close modal when clicking X
        closeBtn.onclick = function() {
            modal.style.display = 'none';
        }

        // Close modal when clicking outside the image
        modal.onclick = function(event) {
            if (event.target === modal) {
                modal.style.display = 'none';
            }
        }

        // Close modal with Escape key
        document.addEventListener('keydown', function(event) {
            if (event.key === 'Escape' && modal.style.display === 'block') {
                modal.style.display = 'none';
            }
        });
    </script>
</body>
</html>
