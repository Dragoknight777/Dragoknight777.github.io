<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Project 3A — Image Warping & Mosaicing</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="./style.css" />
  <!-- MathJax: render LaTeX equations on the page -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['\\(','\\)'], ['$', '$']], displayMath: [['$$','$$'], ['\\[','\\]']] },
      options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'] }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
</head>
<body>

<header>
  <h1>Image Warping and Mosaicing (Proj 3A)</h1>
  <p>CS180/280A • Fall 2025</p>
</header>

<section id="overview">
  <h2>Overview</h2>
  <p>
    I implement planar homography estimation from point correspondences, inverse warping with
    both nearest-neighbor and bilinear interpolation, and simple feather blending to stitch pairs
    of photos into panoramas. The code builds the DLT system for a homography, solves it via the
    smallest eigenvector of \(A^\top A\), predicts the canvas bounds by transforming image corners,
    and blends overlapping regions with a distance-to-edge weight (gamma falloff).
  </p>
</section>

<section id="data" >
  <h2>A.1 Shoot & Digitize</h2>
  <p>
    I captured three scenes with significant overlap: Hearst, Parking Lot, and Kitchen.
    Each set consists of a view that I warp and one that I warp to.
  </p>
  <div class="figure-grid two-columns">
    <!-- Replace with your actual originals -->
    <div class="figure">
      <img src="images/hearstleft.jpg" alt="Hearst left" />
      <div class="caption">Hearst — left</div>
    </div>
    <div class="figure">
      <img src="images/hearstright.jpg" alt="Hearst right" />
      <div class="caption">Hearst — right</div>
    </div>

    <div class="figure">
      <img src="images/parkingleft.jpg" alt="Parking left" />
      <div class="caption">Parking — left</div>
    </div>
    <div class="figure">
      <img src="images/parkingright.jpg" alt="Parking right" />
      <div class="caption">Parking — right</div>
    </div>

    <div class="figure">
      <img src="images/kitchenleft.jpg" alt="Kitchen left" />
      <div class="caption">Kitchen — left</div>
    </div>
    <div class="figure">
      <img src="images/kitchenright.jpg" alt="Kitchen right" />
      <div class="caption">Kitchen — right</div>
    </div>
  </div>
</section>

<section id="homography">
  <h2>A.2 Recover Homographies</h2>
  <p>
    I form the standard DLT matrix with two rows per correspondence \((x,y)\!\leftrightarrow\!(u,v)\), then
    compute the homography as the eigenvector associated with the smallest eigenvalue of \(A^\top A\)
    (reshaped to \(3\times3\) and normalized so \(H_{33}=1\)).
  </p>
  <div class="code">
    # Row pattern used (u,v from right image; x,y from left image):<br/>
    [x, y, 1, 0, 0, 0, -u*x, -u*y, -u]<br/>
    [0, 0, 0, x, y, 1, -v*x, -v*y, -v]
  </div>

  <h3>Recovered matrices</h3>
  <div class="code">
    <div>H_hearst = $$\begin{bmatrix}
      2.88988154e+00 & -4.05352773e-02 & -8.53498499e+03 \\
      7.47392228e-01 & 2.39298908e+00 & -3.46312296e+03 \\
      3.26109171e-04 & 4.19004124e-06 & 1.00000000e+00
    \end{bmatrix}$$</div>
    <div>H_parking = $$\begin{bmatrix}
      5.20478150e-01 & 5.98155432e-02 & 1.46618912e+03 \\
      -2.11955367e-01 & 8.91271245e-01 & 1.91033781e+02 \\
      -1.27856278e-04 & 2.07037962e-05 & 1.00000000e+00
    \end{bmatrix}$$</div>
    <div>H_kitchen = $$\begin{bmatrix}
      1.98697043e+00 & 2.12945343e-02 & -4.24030075e+03 \\
      3.56015356e-01 & 1.62764767e+00 & -1.25441368e+03 \\
      1.71525680e-04 & -7.36226937e-06 & 1.00000000e+00
    \end{bmatrix}$$</div>
  </div>
</section>

<section id="warping">
  <h2>A.3 Warping (Inverse Mapping + Interpolation)</h2>
  <p>
    Warping uses inverse mapping: for each pixel on the output canvas, map back through
    \(H^{-1}\) to sample the source. I implemented the warp using both nearest-neighbor and
    bilinear interpolation.
    The canvas bounds are computed by transforming the left image corners with \(H\), then unioning
    with the right image extent to set <code>(min_x,min_y,max_x,max_y)</code>.
  </p>

  <h3>Interpolation comparison</h3>
  <div class="figure-grid two-columns">
    <!-- Hearst -->
    <div class="figure">
      <img src="images/hearst_warpedleft_nn.jpg" alt="Hearst warped left (NN)" />
      <div class="caption">Hearst — warped left (nearest neighbor)</div>
    </div>
    <div class="figure">
      <img src="images/hearst_warpedleft_bl.jpg" alt="Hearst warped left (bilinear)" />
      <div class="caption">Hearst — warped left (bilinear)</div>
    </div>

    <!-- Parking -->
    <div class="figure">
      <img src="images/parking_warpedleft_nn.jpg" alt="Parking warped left (NN)" />
      <div class="caption">Parking — warped left (nearest neighbor)</div>
    </div>
    <div class="figure">
      <img src="images/parking_warpedleft_bl.jpg" alt="Parking warped left (bilinear)" />
      <div class="caption">Parking — warped left (bilinear)</div>
    </div>

    <!-- Kitchen -->
    <div class="figure">
      <img src="images/kitchen_warpedleft_nn.jpg" alt="Kitchen warped left (NN)" />
      <div class="caption">Kitchen — warped left (nearest neighbor)</div>
    </div>
    <div class="figure">
      <img src="images/kitchen_warpedleft_bl.jpg" alt="Kitchen warped left (bilinear)" />
      <div class="caption">Kitchen — warped left (bilinear)</div>
    </div>
  </div>

  <h3>Notes on speed</h3>
  <ul>
    <li>Nearest neighbor is certainly faster</li>
    <li>Nearest Neighbor vs Bilinear times:</li>
    <li>Hearst: NN ~35s, Bilinear ~100s</li>
    <li>Parking: NN ~8.8s, Bilinear ~18s</li>
    <li>Kitchen: NN ~17s, Bilinear ~36s</li>
    <li>To be honest I can't really notice any difference between the nearest neighbor and bilinear produced images.</li>
  </ul>
</section>

<section id="mosaics">
  <h2>A.4 Mosaics + Feather Blending</h2>
  <p>
    I assemble panoramas by placing the right image on a canvas, warping the left image with the
    recovered \(H\), and blending where they overlap using a per-pixel alpha based on distance to
    the image border. The alpha is <code>(distance_to_edge / max_distance) ^ gamma</code>. Finally, I normalize by the sum of alphas
    to avoid seams.
  </p>

  <h3>Scene 1 — Hearst</h3>
  <div class="figure-grid">
    <div class="figure">
      <img src="images/hearst_panorama_nn.jpg" alt="Hearst panorama NN" />
      <div class="caption">Panorama (nearest neighbor sampling)</div>
    </div>
    <div class="figure">
      <img src="images/hearst_panorama_bilinear.jpg" alt="Hearst panorama bilinear" />
      <div class="caption">Panorama (bilinear sampling)</div>
    </div>
  </div>

  <h3>Scene 2 — Parking Lot</h3>
  <div class="figure-grid">
    <div class="figure">
      <img src="images/parking_panorama_nn.jpg" alt="Parking panorama NN" />
      <div class="caption">Panorama (nearest neighbor sampling)</div>
    </div>
    <div class="figure">
      <img src="images/parking_panorama_bl.jpg" alt="Parking panorama bilinear" />
      <div class="caption">Panorama (bilinear sampling)</div>
    </div>
  </div>

  <h3>Scene 3 — Kitchen</h3>
  <div class="figure-grid">
    <div class="figure">
      <img src="images/kitchen_panorama_nn.jpg" alt="Kitchen panorama NN" />
      <div class="caption">Panorama (nearest neighbor sampling)</div>
    </div>
    <div class="figure">
      <img src="images/kitchen_panorama_bl.jpg" alt="Kitchen panorama bilinear" />
      <div class="caption">Panorama (bilinear sampling)</div>
    </div>
  </div>

  <h3>Blending details</h3>
  <ul class="tight">
    <li>Alpha per image: distance-transform–style falloff from center to edges (implemented as min-distance to border).</li>
    <li>Final pixel = weighted average by alpha sums; prevents hard seams in overlap.</li>
    <li>Gamma controls edge softness; I used ~1.5–1.8.</li>
  </ul>
</section>

<section id="b1">
  <h2>Part B.1 — Harris Corner Detection &amp; ANMS</h2>

  <p>
    Harris corner detection finds repeatable “interest points” by looking for pixels where intensity changes strongly in two directions. After computing a corner strength map, we keep only the local peaks. However, raw peaks often clump together in textured areas, so we apply Adaptive Non-Maximal Suppression (ANMS): for each detected corner we measure how far you’d have to go to find a noticeably stronger one, and treat that distance as its “radius.” Sorting by this radius and keeping the top N gives a set of points that are both strong and well spread out across the image.
  </p>

  <!-- Harris responses: left vs right -->
  <h3>Harris: All Detected Corners</h3>
  <div class="compare">
    <figure class="figure">
      <img src="./images2/outputs_b1/kitchen_left_harris.png" alt="Harris corners — left image" class="img-full img-shadow">
      <figcaption class="caption">Harris corners (left image).</figcaption>
    </figure>
    <figure class="figure">
      <img src="./images2/outputs_b1/kitchen_right_harris.png" alt="Harris corners — right image" class="img-full img-shadow">
      <figcaption class="caption">Harris corners (right image).</figcaption>
    </figure>
  </div>

  <!-- ANMS selections: left vs right -->
  <h3>Adaptive Non-Maximal Suppression (ANMS)</h3>
  <div class="compare">
    <figure class="figure">
      <img src="./images2/outputs_b1/kitchen_left_anms.png" alt="ANMS corners — left image" class="img-full img-shadow">
      <figcaption class="caption">Top-1000 corners after ANMS (left image).</figcaption>
    </figure>
    <figure class="figure">
      <img src="./images2/outputs_b1/kitchen_right_anms.png" alt="ANMS corners — right image" class="img-full img-shadow">
      <figcaption class="caption">Top-1000 corners after ANMS (right image).</figcaption>
    </figure>
  </div>
</section>

<!-- =========================
     B.2 — Feature Descriptor Extraction
     ========================= -->
<section id="b2">
  <h2>Part B.2 — Feature Descriptor Extraction</h2>

  <p>
    Around each ANMS-selected corner, a <strong>40 × 40 pixel</strong> window is cropped, 
    blurred slightly, and resampled to an <strong>8 × 8</strong> patch. 
    Each patch vector \(p\) is then <em>bias / gain normalized</em> to achieve illumination invariance:
  </p>

  <p class="eq">\(p' = \dfrac{p - \mu(p)}{\sigma(p) + \epsilon}\)</p>

  <!-- <p>
    These descriptors capture coarse gradient patterns and intensity structure, enabling 
    robust correspondence under lighting changes or minor perspective variations.
    Below are 20 sample patches (top features by Harris score) from each image in the <em>kitchen</em> pair.
  </p> -->


  <p>
    Around each ANMS point, we capture a compact description of the local appearance by cropping a 40×40 pixel window, downsampling it to 8×8, and normalizing it to remove overall brightness and contrast. The result is a 64-value vector that summarizes the coarse pattern of edges and intensities near that point. These lightweight descriptors are simple but effective: they are fast to compute, reasonably robust to lighting changes, and comparable between different images of the same scene.
  </p>

  <div class="figure-vertical">
    <figure class="figure">
      <img src="./images2/outputs_b2/kitchen_left_descriptors_grid.png"
           alt="8×8 descriptor patches — left image"
           class="img-full img-shadow">
      <figcaption class="caption">Top 20 8×8 normalized descriptors from the left image.</figcaption>
    </figure>

    <figure class="figure">
      <img src="./images2/outputs_b2/kitchen_right_descriptors_grid.png"
           alt="8×8 descriptor patches — right image"
           class="img-full img-shadow">
      <figcaption class="caption">Top 20 8×8 normalized descriptors from the right image.</figcaption>
    </figure>
  </div>
</section>

<!-- =========================
     B.3 — Feature Matching (Lowe Ratio)
     ========================= -->
<section id="b3">
  <h2>Part B.3 — Feature Matching</h2>

  <p>
    Descriptors from the two images are matched using Euclidean distance
    <span class="eq">\(d(x,y)=\lVert x-y\rVert_2\)</span>. For each descriptor in the left image,
    we find the nearest and second-nearest neighbors in the right image and apply the
    <em>Lowe ratio</em> cutoff based on 
    <span class="eq">\(\frac{e_{1-NN}}{e_{2-NN}}\)</span>, where I cutoff any images with a Lowe ratio above 0.7.
    This suppresses ambiguous matches in repeated textures while keeping distinctive correspondences.
    The figure shows the best matches (sorted by ratio); each cell displays the 8×8 patch from
    each image and its ratio.
  </p>

  <figure class="figure figure-narrow">
    <img src="./images2/outputs_b3/kitchen_matched_patch_gallery.png"
         alt="Matched 8×8 patch gallery with Lowe ratios"
         class="img-full img-shadow">
    <figcaption class="caption">
      Kitchen pair — matched 8×8 patches (left vs right) with Lowe ratios; lower is better.
    </figcaption>
  </figure>
</section>

<!-- =========================
     B.4 — RANSAC for Robust Homography
     ========================= -->
<section id="b4">
  <h2>Part B.4 — RANSAC for Robust Homography</h2>

  <p>
    The tentative matches include some mistakes, so we use RANSAC to estimate a reliable homography. 
    RANSAC repeatedly samples four matches, computes a candidate warp, and counts how many other matches 
    agree with it within a small pixel error. The model with the most agreement is then recomputed from 
    all its inliers and used to warp and blend the images into a panorama. The inlier visualizations 
    highlight only the correspondences that support the final mosaic. I am actually quite surprised by 
    how well the corners matched even though a lot of corners for the hearst and parking lot images
    were in trees. 
  </p>


  <!-- ===== KITCHEN ===== -->
  <h3>Kitchen</h3>
  <div class="compare">
    <figure class="figure">
      <img src="./images2/outputs_b4/kitchen_left_inliers.png" alt="Kitchen — inlier keypoints (left)" class="img-full img-shadow">
      <figcaption class="caption">Inlier correspondences — left image.</figcaption>
    </figure>
    <figure class="figure">
      <img src="./images2/outputs_b4/kitchen_right_inliers.png" alt="Kitchen — inlier keypoints (right)" class="img-full img-shadow">
      <figcaption class="caption">Inlier correspondences — right image.</figcaption>
    </figure>
  </div>
  <figure class="figure figure-wide">
    <img src="./images2/outputs_b4/kitchen_mosaic.png" alt="Kitchen mosaic" class="img-full img-shadow">
    <figcaption class="caption">Final mosaic (Kitchen) using the robust homography.</figcaption>
  </figure>

  <!-- ===== PARKING ===== -->
  <h3>Parking</h3>
  <div class="compare">
    <figure class="figure">
      <img src="./images2/outputs_b4/parking_left_inliers.png" alt="Parking — inlier keypoints (left)" class="img-full img-shadow">
      <figcaption class="caption">Inlier correspondences — left image.</figcaption>
    </figure>
    <figure class="figure">
      <img src="./images2/outputs_b4/parking_right_inliers.png" alt="Parking — inlier keypoints (right)" class="img-full img-shadow">
      <figcaption class="caption">Inlier correspondences — right image.</figcaption>
    </figure>
  </div>
  <figure class="figure figure-wide">
    <img src="./images2/outputs_b4/parking_mosaic.png" alt="Parking mosaic" class="img-full img-shadow">
    <figcaption class="caption">Final mosaic (Parking) using the robust homography.</figcaption>
  </figure>

  <!-- ===== HEARST ===== -->
  <h3>Hearst</h3>
  <div class="compare">
    <figure class="figure">
      <img src="./images2/outputs_b4/hearst_left_inliers.png" alt="Hearst — inlier keypoints (left)" class="img-full img-shadow">
      <figcaption class="caption">Inlier correspondences — left image.</figcaption>
    </figure>
    <figure class="figure">
      <img src="./images2/outputs_b4/hearst_right_inliers.png" alt="Hearst — inlier keypoints (right)" class="img-full img-shadow">
      <figcaption class="caption">Inlier correspondences — right image.</figcaption>
    </figure>
  </div>
  <figure class="figure figure-wide">
    <img src="./images2/outputs_b4/hearst_mosaic.png" alt="Hearst mosaic" class="img-full img-shadow">
    <figcaption class="caption">Final mosaic (Hearst) using the robust homography.</figcaption>
  </figure>

  

  
</section>


<footer style="margin-top:40px; font-size:.9rem; color:#666">
  <p>Course spec: CS180/280A Project 3A — Image Warping & Mosaicing.</p>
</footer>

</body>
</html>
